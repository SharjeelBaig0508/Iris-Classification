{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "iris_model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUisLn6za_qW",
        "colab_type": "text"
      },
      "source": [
        "# ASSIGNMENT : IRIS MULTI-CLASS CLASSIFICATION\n",
        "\n",
        "###### Purpose :\n",
        "To predict the species of flower .\n",
        "###### Description :\n",
        "The dataset contains a set of 150 records under 5 attributes - Petal Length, Petal Width, Sepal Length, Sepal width and Class(Species).\n",
        "###### Requirements :\n",
        "1) Code must be in tf 2.0 .\n",
        "\n",
        "2) Accuracy must be in between 95-97% .\n",
        "\n",
        "3) Model shouldn't be Overfit (You can add drop out layer for this) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x1NRl-fa_qg",
        "colab_type": "text"
      },
      "source": [
        "### STEP 1 : Load all the necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PzkaQDDa_ql",
        "colab_type": "code",
        "outputId": "b099b656-72a1-4b0e-f1c5-75830578dd87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "if tf.__version__ < \"2.0.0\":\n",
        "  !pip install --upgrade tensorflow_gpu==2.0\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49resuTCa_q3",
        "colab_type": "text"
      },
      "source": [
        "### STEP 2 : Data Preparation\n",
        "This step consists of multiple sub steps from data loading [download](https://github.com/ramsha275/PIAIC-Sir-Anees-Quarter-2/blob/master/Deep%20Learning/iris.csv),shuffling ,spliting in **Train** and **Test** sets to one-hot-enconding on labels . \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fymrwawba_q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_url = \"https://github.com/ramsha275/PIAIC-Sir-Anees-Quarter-2/raw/master/Deep%20Learning/iris.csv\"\n",
        "data_file_iris = pd.read_csv(data_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZanyFt0hhCdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampler = np.random.permutation(150)\n",
        "shuffled_file_iris = data_file_iris.take(sampler)\n",
        "train_data = shuffled_file_iris[:120]\n",
        "test_data = shuffled_file_iris[120:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgndKw05kyQ1",
        "colab_type": "code",
        "outputId": "ea23a47e-c31b-4ef8-dcbb-866f01dcd893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "column_names = list(data_file_iris.columns)\n",
        "print(column_names)\n",
        "features_names = column_names[:-1]\n",
        "label_name = column_names[-1]\n",
        "print(features_names)\n",
        "print(label_name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sepal.length', 'sepal.width', 'petal.length', 'petal.width', 'variety']\n",
            "['sepal.length', 'sepal.width', 'petal.length', 'petal.width']\n",
            "variety\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZVMnFTvl1Qs",
        "colab_type": "code",
        "outputId": "67f39890-4118-42ed-dc4c-c24ae0d57e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_names = data_file_iris['variety'].unique()\n",
        "print(class_names)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Setosa' 'Versicolor' 'Virginica']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzFfMmwMzsRo",
        "colab_type": "code",
        "outputId": "4bd649aa-3e4d-46b4-80bd-3f55c6cf81ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_features, train_labels = train_data[features_names], train_data[label_name]\n",
        "test_features, test_labels = test_data[features_names], test_data[label_name]\n",
        "print(train_features)\n",
        "print(train_labels)\n",
        "print(test_features)\n",
        "print(test_labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     sepal.length  sepal.width  petal.length  petal.width\n",
            "60            5.0          2.0           3.5          1.0\n",
            "1             4.9          3.0           1.4          0.2\n",
            "71            6.1          2.8           4.0          1.3\n",
            "12            4.8          3.0           1.4          0.1\n",
            "21            5.1          3.7           1.5          0.4\n",
            "..            ...          ...           ...          ...\n",
            "121           5.6          2.8           4.9          2.0\n",
            "92            5.8          2.6           4.0          1.2\n",
            "37            4.9          3.6           1.4          0.1\n",
            "59            5.2          2.7           3.9          1.4\n",
            "109           7.2          3.6           6.1          2.5\n",
            "\n",
            "[120 rows x 4 columns]\n",
            "60     Versicolor\n",
            "1          Setosa\n",
            "71     Versicolor\n",
            "12         Setosa\n",
            "21         Setosa\n",
            "          ...    \n",
            "121     Virginica\n",
            "92     Versicolor\n",
            "37         Setosa\n",
            "59     Versicolor\n",
            "109     Virginica\n",
            "Name: variety, Length: 120, dtype: object\n",
            "     sepal.length  sepal.width  petal.length  petal.width\n",
            "2             4.7          3.2           1.3          0.2\n",
            "55            5.7          2.8           4.5          1.3\n",
            "15            5.7          4.4           1.5          0.4\n",
            "117           7.7          3.8           6.7          2.2\n",
            "98            5.1          2.5           3.0          1.1\n",
            "146           6.3          2.5           5.0          1.9\n",
            "41            4.5          2.3           1.3          0.3\n",
            "137           6.4          3.1           5.5          1.8\n",
            "91            6.1          3.0           4.6          1.4\n",
            "88            5.6          3.0           4.1          1.3\n",
            "85            6.0          3.4           4.5          1.6\n",
            "25            5.0          3.0           1.6          0.2\n",
            "46            5.1          3.8           1.6          0.2\n",
            "94            5.6          2.7           4.2          1.3\n",
            "65            6.7          3.1           4.4          1.4\n",
            "134           6.1          2.6           5.6          1.4\n",
            "136           6.3          3.4           5.6          2.4\n",
            "19            5.1          3.8           1.5          0.3\n",
            "115           6.4          3.2           5.3          2.3\n",
            "0             5.1          3.5           1.4          0.2\n",
            "78            6.0          2.9           4.5          1.5\n",
            "49            5.0          3.3           1.4          0.2\n",
            "76            6.8          2.8           4.8          1.4\n",
            "140           6.7          3.1           5.6          2.4\n",
            "56            6.3          3.3           4.7          1.6\n",
            "45            4.8          3.0           1.4          0.3\n",
            "135           7.7          3.0           6.1          2.3\n",
            "74            6.4          2.9           4.3          1.3\n",
            "7             5.0          3.4           1.5          0.2\n",
            "66            5.6          3.0           4.5          1.5\n",
            "2          Setosa\n",
            "55     Versicolor\n",
            "15         Setosa\n",
            "117     Virginica\n",
            "98     Versicolor\n",
            "146     Virginica\n",
            "41         Setosa\n",
            "137     Virginica\n",
            "91     Versicolor\n",
            "88     Versicolor\n",
            "85     Versicolor\n",
            "25         Setosa\n",
            "46         Setosa\n",
            "94     Versicolor\n",
            "65     Versicolor\n",
            "134     Virginica\n",
            "136     Virginica\n",
            "19         Setosa\n",
            "115     Virginica\n",
            "0          Setosa\n",
            "78     Versicolor\n",
            "49         Setosa\n",
            "76     Versicolor\n",
            "140     Virginica\n",
            "56     Versicolor\n",
            "45         Setosa\n",
            "135     Virginica\n",
            "74     Versicolor\n",
            "7          Setosa\n",
            "66     Versicolor\n",
            "Name: variety, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUQdLL0hT0mt",
        "colab_type": "code",
        "outputId": "89b41305-3f11-4626-9668-f0b98a3e0ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Label encode Class (Species)\n",
        "encoder = LabelEncoder()\n",
        "new_label = encoder.fit_transform(class_names)\n",
        "print(new_label)\n",
        "# One Hot Encodeone\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "new_label = new_label.reshape(len(new_label), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(new_label)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2]\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fq1E0wDT9-k",
        "colab_type": "code",
        "outputId": "94568ec4-0b5d-4ba1-cbd2-bcb8361b0bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# plt.plot(features['petal.length'], features['sepal.length'])\n",
        "plt.scatter(train_features['petal.length'],\n",
        "            train_features['sepal.length'],\n",
        "            cmap='viridis')\n",
        "plt.scatter(test_features['petal.length'],\n",
        "            test_features['sepal.length'],\n",
        "            cmap='viridis')\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Sepal length\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de5xV9Xnv8c8zw6CjSaDIWHG4KbEY\nKyow3jqeVKWNNVVC0XpJPamahJiLaEzoC1JKEqsHGpJojK9GOdpcjgkJGkWiJqYHTKM0ergpaKJJ\nNOIwahi0YtSpwPCcP/Ya2Je196x9Wfu2vu/Xa17M/u211v4t2Mwzez3Pen7m7oiISHK11HoCIiJS\nWwoEIiIJp0AgIpJwCgQiIgmnQCAiknDDaj2BYo0ePdonTpxY62mIiDSUDRs27HD3jrDnGi4QTJw4\nkfXr19d6GiIiDcXMtuZ7LtZLQ2b2aTN7ysyeNLPlZnZg1vMHmNkPzOy3ZvaYmU2Mcz4iIpIrtkBg\nZp3AXKDL3Y8FWoGLsjb7MPBf7v5u4AbgX+Kaj4iIhIs7WTwMaDezYcBBwItZz38A+Hbw/V3ADDOz\nmOckIiJpYgsE7t4LfBl4AXgJ2OnuP83arBPoCbbfA+wEDolrTiIikivOS0N/ROo3/iOAw4GDzeyS\nEo81x8zWm9n6vr6+Sk5TRCTx4rw09BfA79y9z913A3cDf5a1TS8wDiC4fDQCeCX7QO6+zN273L2r\noyO0+klEREoUZ/noC8ApZnYQ0A/MALLrPlcBfw/8AjgfWONqhyoiDWLlpl6WPvgML77Wz+Ej25l3\n1mRmTe2s9bSKFlsgcPfHzOwuYCOwB9gELDOza4H17r4KuB34P2b2W+BVcquKRETq0spNvSy4ewv9\nuwcA6H2tnwV3bwFouGBgjfYLeFdXl+uGMhGpte4la+h9rT9nvHNkO2vnn1mDGRVmZhvcvSvsOfUa\nEhEpwYshQaDQeD1TIBARKcHhI9uLGq9nCgQiIiWYd9Zk2ttaM8ba21qZd9bkGs2odA3XdE5EpB4M\nJoRVNSQikmCzpnY25A/+bLo0JCKScAoEIiIJp0AgIpJwyhGISHJtXgGrr4Wd22DEWJixCI67oCov\nvW7VrYzbuJRDvY/t1kHPtHmcOPNjOdtVo42FAoGIJNPmFfCjubA7uAFsZ0/qMcQeDNatupVjNyyk\n3XaBwWH0MWLDQtZBRjCoVhsLXRoSkWRafe3+IDBod39qPGbjNi5NBYE07baLcRuXZowtffCZfUFg\nUP/uAZY++ExF56NAICLJtHNbceMVdKiHr6tyqO/IeFytNhYKBCKSTCPGFjdeQdstfF2V7TY643G1\n2lgoEIhIMs1YBG1ZP1Db2lPjFbZyUy/dS9ZwxPz76V6yhp+P/zj9Pjxjm34fTs+0eRlj1WpjoWSx\niCTTYEI45qqhsITv59/8UzhiPu994Rsc6jvYbqPpmZ5bNVStNhZaj0BEJEb1sm6B1iMQEamRRli3\nQIFARCRGjbBugQKBiEiMGmHdgtgCgZlNNrPH075eN7Ors7Y53cx2pm1T+XS9iEgNzZrayeLZU+gc\n2Y6Ryg0snj2lrtpXx1Y15O7PACcAmFkr0AvcE7Lpw+5+TlzzEBGptXpft6Bal4ZmAM+6+9YqvZ6I\niERUrUBwEbA8z3OnmtkTZvZjM/vTsA3MbI6ZrTez9X194bdmi4hIaWIPBGY2HJgJ3Bny9EZggrsf\nD3wdWBl2DHdf5u5d7t7V0RF+a7aIiJSmGncWnw1sdPffZz/h7q+nff+Amf2rmY12z+q8JCISg2r0\n+m8E1QgEF5PnspCZHQb83t3dzE4i9QnllSrMSUQSrlq9/htBrJeGzOxg4C+Bu9PGrjCzK4KH5wNP\nmtkTwE3ARd5oPS9EpCFVq9d/I4j1E4G7vwkckjV2S9r3NwM3xzkHEZEwjdD6oVp0Z7GIJFIjtH6o\nFgUCEUmkRmj9UC1aj0BEEqmYXv/NXl2kQCAiiRWl9UMSqot0aUhEpIAkVBcpEIiIFJCE6iIFAhGR\nApJQXaQcgYg0tKiJ3LDtYOhk8byzJmfkCKD5qosUCESkYUVN5IZtN+/OJ8Bg94AX3LeY6qJGpUAg\nIg2rUCI3/Qd12Ha79+Z2swnbF+p/YZlyKUcgIg0raiK3mMRuMyWBo1IgEJGGFTWRW0xit5mSwFEp\nEIhIw4raJiJsu7YWo63Vhtw3CZQjEJGGlS+RC9C9ZE3G2OLZU0qqGkoCa7T2/11dXb5+/fpaT0NE\n6lR2hRCkftNfPHtKIn/IDzKzDe7eFfacLg2JSFNJQkuISlMgEJGmkoSWEJWmQCAiTSUJLSEqLbZA\nYGaTzezxtK/XzezqrG3MzG4ys9+a2WYzmxbXfESSaOWmXrqXrOGI+ffTvWQNKzf11npKsdOCM8WL\nrWrI3Z8BTgAws1agF7gna7OzgaOCr5OBbwR/ikiZktBHP0wSWkJUWrXKR2cAz7r71qzxDwDf8VTp\n0qNmNtLMxrj7S1Wal0jTitp+oRk1e0uISqtWjuAiYHnIeCfQk/Z4WzCWwczmmNl6M1vf19cX0xRF\nmouSphJV7IHAzIYDM4E7Sz2Guy9z9y537+ro6Kjc5ESamJKmElU1Lg2dDWx099+HPNcLjEt7PDYY\nE5EyFdNHv5jF2ctZyL3cReCbfRH5WqlGILiY8MtCAKuAT5nZ90kliXcqPyBSGVGTpsUklctJQJeb\nvE5q8rsaYm0xYWYHAy8AR7r7zmDsCgB3v8XMDLgZ+CvgLeAydy/YP0ItJkQqq3vJGnpD8gadI9tZ\nO//Mkrct53Xi2D/pCrWYiPUTgbu/CRySNXZL2vcOfDLOOYhIYcUklctJQJebvFbyOz66s1gk4YpJ\nKpeTgC43ea3kd3wUCEQSrpg7ccu5a7fcO351x3B8tB6BSNJsXgGrr4Wd22DEWGbNWASzuyNV45Rz\n124xaweU+9qqLiqO1iMQSZLNK+BHc2F32nX1tnY49yY47oKqTyeOtQO0HkE4rUcgIimrr80MApB6\nvPramkwnjrUDtB5B8RQIRJJk57bixmMWRyWQqouKp0AgkiQjxhY3HrM4KoFUXVQ8JYtF6l1WcpcZ\niyJfz89Omt54zJWcuOXzuTmCGYsi7V/p5Gy+Nhg3HvMbuGFuzjkvXLmF5Y/1MOBOqxkXnzyOrgmj\nMl77jKM7+OGG3kitNSRFyWKRelZGcjdf0vQ7J27lxGe/PmRgiZp0LTc5mxusfhMarFaMmcc//Pro\nnP1bgL1pj9vbWjlveicPPd2nqqE0hZLFCgQi9eyGY2FnT+74iHHw6ScL7lqtlg4Vb/2Q55x7fTTd\nb98U6RBqO5FLVUMijaqM5G61WjpUPDmb59zG8ErkQygxXBwFApF6VkZyt1otHSqenM1zbi9lti0r\nSInh4igQiNSzGYtSOYF0BZK76arV0qHirR/ynPPaCZ8I3Tz7h5gSw8VTIBCpZ8ddwLopX+RlOtjr\nxst0sG7KF/Mmd7uXrOGI+ffTvWQNAItnT6FzZDtG6rp5MXfXzpraGWn/qNsVc86ce1MqD4Kl/jz3\nJi64/DNccsp4Ws0AaDXjklPG89ULT6jcayfUkMliM+sGvgBMIFVuaqQ6SB8Z++xCKFksSVKtyh1p\nfuUmi28HvgqcBpwIdAV/ikjMorZLUFsFKUeUG8p2uvuPY5+JiOSoWeWOJEreQGBm04JvHzKzpcDd\nwNuDz7v7xpjnJpJ4h49sD63RD6vcibKdSJhCnwi+kvU4/dqSA0PerWFmI4HbgGODfS5391+kPX86\ncC/wu2DobnevTRtEkTqUrwVDWOXOvLueYPfA/pxfW6vFUj0T1k4CSlujIN/xislraO2B8uUNBO5+\nBoCZHenuz6U/Z2ZRE8VfA37i7ueb2XDgoJBtHnb3c6JOWCRJiloIJrvuI4amAdlJ6d7X+pl31xPg\nsHuv7xtbcPeWjPkXc7yo+1Zif0mJkiO4C5iWNXYnML3QTmY2AngvcCmAu+8CdhU/RZFkmzW1c8gf\naksffGbfD+JBu/c6Sx98pqI/EMOS0umfQgYNJqqjzDtfkjvKvMvdX1IK5QiOBv4UGGFms9Oeehdw\nYIRjHwH0Ad80s+OBDcBV7v5m1nanmtkTwIvAZ939qZC5zAHmAIwfPz7CS4skS7WSxcUcL8q21WqD\nIYUVKh+dDJwDjATOTfuaBnw0wrGHBdt+w92nAm8C87O22QhMcPfjga8DK8MO5O7L3L3L3bs6Ojoi\nvLRIslSrB38xx4uybbXaYEhheQOBu9/r7pcB57j7ZWlfc939PyMcexuwzd0fCx7nXGJy99fd/Y3g\n+weANjMbXdqpiCRXxds8FPE6ba1GW4uV9NrVaoMhhUXJEXzQzC7OGtsJrHf3e/Pt5O4vm1mPmU12\n92eAGcAv07cxs8OA37u7m9lJpAJT9BaDIs0m4iI0YZUyi2dPib16Jl/yOmyss+c+Xr53KYd6H9ut\ng55p8zhx5sdyjtfZcx/jNmZud+/WTj6z4omMBWiumzUl8nyUHyhOlBYTy4CjSSWIAc4jVe55CPCc\nu19dYN8TSJWPDgeeAy4DLgRw91vM7FPAx4E9QD9wzVCfNtRiQppWxEVoGqGdxLpVt3LshoW02/76\nkH4fzpPTr8sMBiHnvMsO4LNvf5hVe0/LOOYlp4wPDQYSTVkL05jZo0C3uw8Ej4cBD5NqObHF3Y+p\n8HwLUiCQphVxEZqKLwQTg5e/8G4Ooy93nA4O+8Jv9w/kOedte0dz2q7MRWhazXh28fsrPtekKLfX\n0B8B70h7fDAwKggMb4fvIiJFi7gITSNUyhzquUEgNb4jcyDPOR9uuVeIBxpsNcVGEiUQfAl43My+\naWbfAjYBS83sYOD/xjk5kUSJuAhNI1TKbLfw6r7t2bUgec75Rc9dhGaw/bRU3pDJYne/3cweAE4K\nhj7n7i8G38+LbWYiSTNjUWiOYN2kK7l6yZp9ydAzju7ghxt6h2w7AUROPi9cuYXlj/XkJmdD9l85\n0D1kcrZn2jxGhOQIeqbP47AhznmXHcDqvSfwyPC5HG47eNFH86U9F/Cukz5Y9F9pOrWiyC/S4vVm\n1sn+9QgAcPefxzivvJQjkKaW9YN33aQr+dC6CTk/9M+b3slDT/cV/qEWMfm8cOUW7nj0hZypfOlP\nnuaCl5Zm7L+n9UDm7/4Id+36s4z5hCWq1626NagG2sF2Gx1aNRR2zs+O7GbM83dzUFoQecuH81R2\norkIjZBgj1u5yeJ/IVXp8xSwNxh2d59Z0VlGpEAgSVJWYjhi8nnSggdCr7+vPWAunbYjZzwskVvJ\nRHXkRHMRGiHBHrdCgSDKfQSzgMnursSwSJWVlRiOmHzOl4QdQ24QgPBEbiUT1Yd6X2odxJzx8PlE\n0QgJ9lqKkix+DmiLeyIikqusxHDE5HO+JOxLhN/kH5bIrWSiOnKiuQiNkGCvpSiB4C1SVUO3mtlN\ng19xT0xEymyhMGNRKieQrq09NZ7m4pPHhe6+dsIncvbf03ogN3JRafOJqGfaPPp9eMZYvw+nZ1rp\ntSlqRVFYlEtDq4IvEYlZ1NYRkLruXTBZPJgQDqv6ydoXyKkaumDWX8PmcRn7D5uxiNMGuvlFjIvQ\nnDjzY6yDzETz9DyJ5ojUiqKwqFVD7cD4oGdQTSlZLM0qamVLORUwtayeUeVObZV1Z7GZnQs8Dvwk\neHyCmekTgkiFFVpkpZTtynmNONTytaWwKDmCL5C6mew1AHd/HIi6VKWIRBS1sqWcCphaVs+ocqd+\nRQkEu919Z9bY3tAtRaRkUStbyqmAqWX1jCp36leUZPFTZvZBoNXMjgLmAlEWphFJnHLaGMw7a3Lo\nNfTsypZ82914zG/ghrkZieGFz70nIwl8ypF/xKtv7srZ94yjO4ZOPpcp6vkBkVtjSGVECQRXAv9I\nqtPocuBB4J/jnJRII8pOhva+1s+Cu7cARPqhGrWyJWy7G4/5DSdu+fz+dhA7e9h1z6d4/e0PM+Cp\nvv4D7qx99lW6J43i+Vf68/YuKnbeUUWu3MlujbGzJ/UYFAxiEqlqqJ6oakjqVU3bGJTR17/u2i9E\nbI0hxSmpxYSZ/QjIGyVq1WtIpF7VNBlaRl//ukviRmyNIZVT6NLQl6s2C5EmcPjI9tDfrKuSDB0x\nNvS36Ch9/Ws67zB5ziVvywwpW96qIXf/j0JfUQ5uZiPN7C4ze9rMfmVmp2Y9b0HLit+a2WYzm1bu\nCYnUSk3bGIS0k9hlB/ClPbnX1LNbStRd+4WIrTGkcqIki8vxNeAn7n6+mQ0HDsp6/mzgqODrZOAb\nwZ8iDWfW1E7Wb301o0rnvOmd8dw1e981sOFb4ANgrTD90tQ6A2mVNsNnLOJdz72H1qzWEV0TRuVU\nCIW1sZg1tTO0CgpibtWQpzWGEsXxiS1ZbGYjSN2RfKTneREzuxX4mbsvDx4/A5zu7i/lO66SxVKv\nqtZC4b5rYP3tueNdH4ZzvlqxOYZt29Zq4LB7rw+5v9SXchevL9URQB/wTTPbZGa3Bescp+sE0i8G\nbgvGRBpO1VoobPhWceNpiplj2La7BzwjCBTaXxpHnFVDw4BpwJXu/piZfQ2YD/xTsZM0sznAHIDx\n48cXu7tIVVSt+sYHihuPMJew8WLmrTYRjS3OqqFtwDZ3fyx4fBepQJCuF0jPXI0NxjK4+zJgGaQu\nDZU5L5FYVK36xlrDf+hba+5YyFyizjHftvmOK40rtqohd38Z6DGzwdKDGcAvszZbBXwoqB46BdhZ\nKD8gUs+Kqb5ZuamX7iVrOGL+/XQvWcPKTTm//+Q3/dLixrPm2NaSWT7a1mKhcww7n7ZWy/mhkW9/\naRxDVg0F/YUWA8cABw6Ou3uUDqRXAt8NKoaeAy4zsyuC/W8BHgDeD/yW1EpolxV7AiL1ImoLhXJb\nUexLCGdXDQ2RKN4ne2XK8JUqQ8/njKM7+MG6HvYO+JD7S+MYsmrIzB4BPg/cAJxL6od1i7vXpKhX\nVUPS6GrZ0qHc1667dhQSWblVQ+3uvppU0Njq7l8A/rqSExRJkkZeE6Du2lFIRUQJBG+bWQvwGzP7\nlJn9DfCOmOcl0rQaeU0ArSnQnKLcWXwVqTuC55JqP30m8PdxTqqWyuknL8kS9a7b7LHsts9QvZYO\nRa0JkGf/q3/weOi4NK7Idxab2bsAd/c/xDulwuLMEWhxbYkq9K7bFgNL3XS1byzPnbjnTe/koaf7\navILRzm/7Pzd//4Fa599NWe8e9IovvvRU0P2kHpRUhvqtJ27gG8C7wwe7wQud/cNFZ1lHSh016UC\ngaQLvet2b+4vVelBYVD/7gEeerqvZsnVWVNL738UFgQKjUtjiHJp6N+AT7j7wwBmdhqpwHBcnBOr\nBSXCJKpy3xN6T0k9iZIsHhgMAgDu/giwJ74p1Y4SYRJVue8JvaeknkQJBP9hZrea2elm9udm9q/A\nz8xsWrOtH1B3fdmlboXeddtiqZxA+lir5dzJ28jvqe5Jo4oal8YQ5dLQ8cGfn88an0qqKV3T3EUS\neXHtQjaviNRHXdVJ1VXpv+9875WwsbA1CtZvfZXPrHgiY52A62ZNiT6BiO+zcuWrjErPCShR3Pi0\neH0lbV4BP5oLu9Ou/7a1pxYMSftPquqk6qrl33fYa7cYhOSVueSU8dGCQcT3Wbn0Pm0uZd1ZbGZ/\nbGa3m9mPg8fHmNmHKz3JprD62sz/nJB6vPrajKGq9a0XoLZ/32GvHRYEAJY/FrJOb5iI77Ny6X2a\nHFFyBN8CHgQODx7/Grg6rgk1tJ3bIo2rOqm66rGlQ5iBqJ/OI77PyqX3aXJECQSj3X0FsBfA3fcA\nQ6+AkUQjxkYaV3VSddVjS4cwrRaxjWfE91m59D5NjiiB4E0zO4RgtbLBdQNinVWjmrEoda02XVt7\najyNqpOqq9DfdznrAkTZd95Zkzl/+H/yyPC5PHfAB3lk+FxmtT7CzJZHMsZmtjzCxSePC3mVEDMW\nQUtb5lhLW877rBhh56L3aXJEqRq6htQCMpPMbC3QAZwf66wa1WCibohqjopUJ0lkhSp8Sl0XIOqa\nArNa13JO220MG/hvAMbaDr7SugzHGOa79419+YDbGX7kVCBi5VD2p4eonyaKOJfFs6ewePYUvU8T\nIFLVkJkNAyaTWoLiGffgHVwDdV01JA2lnN76kfe94VjYGTEJPGIcfPrJobfLd8yo+2fRGgPJUFLV\nkJmdaGaHwb68wHTgeuArZqa7R6ThlZMMjbxvMQncqNtWOFmspLAUyhHcCuwCMLP3AkuA75DKDyyL\nf2oi8SonGRp532ISuFG3rXCyWElhKRQIWt198PbBC4Fl7v5Dd/8n4N3xT00kXuUkQyPvG1ZA0NIG\nrcMzx0KKCvKKWJQQlZLCUihZ3Gpmw4LLQjOAORH328fMngf+QKrcdE/29SkzOx24F/hdMHS3u1f2\nrpgKiLoAiZJojaVQErl7yZoh/73DEqm5rSPew3Xn3pRbQACRW0Tkvv+6mRV2zBLvKlbxguRNFpvZ\nPwLvB3YA44Fp7u5m9m7g2+7ePeTBU4Ggy9135Hn+dOCz7n5O1AlXO1kcugBJnsVGdOt944u64EzY\nv/fClVu449EXco4ZuXVExPnovSalKClZ7O7XA58hdWfxab4/YrQAV1Z6kvUqdAGSAc9ZhES33jeH\nfAvOZC8wE/bvna9FROTWERHno/eaVFrBSzzu/mjI2K+LOL4DPzUzB25197Ak86lm9gTwIqlPB09l\nb2BmcwguTY0fP76Ily9fMZUTqrJofOX8e+drERG5dUQR89F7TSopyp3F5TjN3acBZwOfDKqP0m0E\nJrj78cDXgZVhB3H3Ze7e5e5dHR0d8c44SzGVE6qyaHzl/HvnaxERuXVEEfPRe00qKdZA4O69wZ/b\ngXuAk7Kef93d3wi+fwBoM7PRcc6pWPPOmpyzsEiLkXexkYUrtzBpwQNMnH8/kxY8wMKVW0KPG3U7\nqa6oC86EVdXkaxERuXVExPm0t7VyxtEdJbfGEMkWWyAws4PNbHDB+4OB9wFPZm1zmFnq1yUzOymY\nzytxzalkWb/QtbYYF540js6R7RipOzAXz57C+q2vcsejL+y7FDDgzh2PvpDzQ34wqTjUdlJ9s6Z2\nsnj2lIx/26V/ezxLzz8+5987O1l73awpXHLK+H2fAFrNykoU55vPedM7+eGGXnpf68fZ3xJCwUBK\nFdvCNGZ2JKlPAZDKRXzP3a83sysA3P0WM/sU8HFSayD3A9e4+38WOm61q4aKuf1+0oIHQq8Ht5rx\n7OL3F72dSBi1hJBSFKoainQ/QCnc/Tn2L3OZPn5L2vc3AzfHNYdKKCZZFzVZGEdSUZJDCWSptLiT\nxQ2vmGRd1GRhHElFSQ4lkKXSFAiGUMzt91GThXEkFSU51BJCKi22S0PNopjb76+bNYXf9b3B2mdf\n3TfWPWlUTrJw8PHyx3rSWhGMKyupKMmhlhBSabEli+NSz+sRqB2AiNSrklpMSPHUDkBEGpECQQWp\nmkNEGpECQQWpmkNEGpGSxSUKW6Ng3lmTueYHj7M3bbsWUDVHkwp7DygXJI1InwhKMJgUzr7F/871\nL2QEAYC9wPqtr4YcRRpZvveA2jxII1IgKEG+pHB62Wi6cvrRS31SYYA0EwWCEhSb/FXriOajwgBp\nJgoEJSg2+avWEc1HhQHSTBQISpDvFv/uSaNCt1friOajNg/STFQ1VIJZUzvZten7dG/9V8awg5cY\nzdoJn+CCyz/D/1ryRT701nc43Hbwoo/mOwd9iM8d+SbccDHs3AYjxsKMRawc6I5UcZKIypTNK2D1\ntRl/Pxx3Qa1nVZDaPEgzUYuJEqxbdSvHblhIu+3aN9bvw3n4oPdx2ls/5aC08V0+jBZzhrE/sbin\n9UDm7/4Id+36s31jYa0oEtGyYvMK+NFc2J12bb2tHc69qe6DgUgjUYuJChu3cWlGEABot12c+dYD\nGUEAYLjtyQgCAMMG/pur+X7GWFjFSSIqU1ZfmxkEIPV49bW1mY9IAikQlOBQ7wsdb825iyC/wy13\nRc7sipNEVKbs3FbcuIhUnAJBCbZbR+j4QBF/nS/6ITlj2RUniahMGTG2uHERqbhYk8Vm9jzwB2AA\n2JN9fSpYuP5rwPuBt4BL3X1jnHMaSlhyFjKTgleN/zgf2Ho9B9j+yzZveyv/cdDZ/PlbP84Y3+MG\nBsPYn4vZzTBu5KKM1w2rOJl31uTQHEFTVabMWBSeI5ixKNLuUZPpiUi6i5SoGlVDZ7j7jjzPnQ0c\nFXydDHwj+LMmspOzva/1M+/OJ8Bg94DvG/vFG68wqzXz3gDDmHToO2h9oQU87bq+Ge4OaZvvdeeI\nUQfR+d/tBX8wJaIyZTAhXELVUNi/14K7twAUTLrn204kqWKtGgo+EXTlCwRmdivwM3dfHjx+Bjjd\n3V/Kd8w4q4a6l6yhN8L190eGz2VsS+4p7aGFYRHzBL0+ms4vPlv0HGW/fP9enSPbWTv/zKK3E2lm\ntawacuCnZrbBzOaEPN8JpDfi2RaMZTCzOWa23szW9/WFJ2orIWoS9nAL/4DT6tGTxWPITRZLcaIm\n0xORdBcpQ9yB4DR3n0bqEtAnzey9pRzE3Ze5e5e7d3V0hCdqKyFqEvZFHx06PmDR/zpfIjdZLMWJ\nmkxPRNJdpAyxBgJ37w3+3A7cA5yUtUkvkN5/YWwwVhNhbQPaWoy2rHzAV/ZeyFs+PGPsLR/O1gkX\nsMsOyBjfxTDe9tacbddO+EQFZ55M886aTFtL5r9NW4uFJt2brR3Eyk29dC9ZwxHz76d7yRq1v5ay\nxBYIzOxgM3vn4PfA+4AnszZbBXzIUk4BdhbKD8Rt1tROFs+eQufIdozUNeSlf3s8S88/PmNs+4Rz\nmb/7I2zbO5q9bmzbO5r5uz/CFf91MZ99+8MZ45/dNYevtF9Fr6fGen00902YzwWXf6ZWp9lcsvv5\nhfT3C/t3beS7s7UWglRabMliMzuS1KcASFUnfc/drzezKwDc/ZagfPRm4K9IlY9e5u4FM8H10GJi\n0oIHimot3WrGs4vfH+OMkimpSeCknreUp1CyOLbyUXd/Djg+ZPyWtO8d+GRcc4hLsesLaD2CeCQ1\nCZzU85b46M7iEhS7voDWI6PfW9IAAAoDSURBVIhHUpPAST1viY8CQQnyrS9w1KEHF7W9lKcZk8BR\nJPW8JT5aj6AE182awpjnV/GBV/9t37oD9466nE9e/Y8sXLmF5Y/1MOBOq9m+IDCYVxgcu27WlEiv\npdYI+SXizusQST1viY/WIyhBvvUInpx+HSfO/FjGtgtXbuGOR1/IOcYlp4wfMhgkYj0CEakKrUdQ\nYfnWIxi3cWnOtssf68kZKzSeLhHrEYhIzSkQlCDfegSHhrRUylcxFKWSSNUhIlINCgQlyLcewXbL\nbT2Rr2IoSiWRqkNEpBoUCKLYvAJuOBa+MBJuOJY3J8ygP6vFRL8Pp2favJxd81UMRakkUnWIiFSD\nAsFQBhdX39kDOOzsYdKL9/LixNm8TAd73XiZjtBEMUDXhFG0ZvXDaW0xuiaMGvKlm601gojUJ1UN\nDeWGY4MgkGXEOPh0duukXGoHICL1QFVD5ShzcXUlfEWk3ikQDKXMxdWV8BWReqdAMJQZi1KLqacr\nYnF1JXxFpN4lpsVEya0a8iyuvvC597B8eW7biLDXWTx7itoBiEjdSkSyuNKtGvK1jeieNIqNL+xU\nSwgRqTuJTxZXulVDvvYQa599VS0hRKThJCIQVLpyp9iFZlQhJCL1LBGBoNKVO8UuNKMKIRGpZ7EH\nAjNrNbNNZnZfyHOXmlmfmT0efH0kjjmUXbmT1WJi8VG/Ct2se9IoVQiJSMOpxieCq4Dwn5wpP3D3\nE4Kv2+KYQFmtGkJaTFzw0lK+9CdP7/tk0GrGJaeM57sfPVUtIUSk4cRaNWRmY4FvA9cD17j7OVnP\nXwp0ufunoh6z0VpMiIjUg1pWDd0I/AOwt8A255nZZjO7y8xCW3Ka2RwzW29m6/v6wtcCiE2ZLSZE\nROpdbIHAzM4Btrv7hgKb/QiY6O7HAf9O6tNDDndf5u5d7t7V0RG+FkBsymwxISJS7+L8RNANzDSz\n54HvA2ea2R3pG7j7K+7+dvDwNmB6jPMpTZktJkRE6l1sgcDdF7j7WHefCFwErHH3S9K3MbMxaQ9n\nUjipXBvHXcCKMfPo9dHsdaPXR7NizDxWDnTTvWQNR8y/n+4la1i5qbfWMxURKUnVew2Z2bXAendf\nBcw1s5nAHuBV4NJqz2coC1du4Y5fHw3ctH/w19Dy68f3JT56X+tnwd1bAFQhJCINJxG9hsoxacED\nke8k1mIzIlKvEt9rqBzFtJNQKwkRaUQKBEMopp2EWkmISCNKdCBYual3yITvxSeH3tqQ8xenVhIi\n0qgSGwgG1yjofa0fZ3/CNzsYXDdrCpecMj6nncRXLzxBrSREpCkkNlncvWQNvSHX9JXwFZFmpGRx\niEqvUSAi0qgSGwgqvUaBiEijSmwgKHuNAhGRJlH1O4vrxWBid+mDz/Dia/0cPrKdeWdNVsJXRBIn\nsYEAUsFAP/hFJOkSe2lIRERSFAhERBJOgUBEJOEUCEREEk6BQEQk4RQIREQSruF6DZlZH7C1jEOM\nBnZUaDq11kznAs11Ps10LtBc55PUc5ng7h1hTzRcICiXma3P13ip0TTTuUBznU8znQs01/noXHLp\n0pCISMIpEIiIJFwSA8GyWk+ggprpXKC5zqeZzgWa63x0LlkSlyMQEZFMSfxEICIiaRQIREQSLjGB\nwMz+zcy2m9mTtZ5LucxsnJk9ZGa/NLOnzOyqWs+pVGZ2oJn9PzN7IjiXL9Z6TuUys1Yz22Rm99V6\nLuUys+fNbIuZPW5m5S8WXmNmNtLM7jKzp83sV2Z2aq3nVAozmxz8mwx+vW5mV5d8vKTkCMzsvcAb\nwHfc/dhaz6ccZjYGGOPuG83sncAGYJa7/7LGUyuamRlwsLu/YWZtwCPAVe7+aI2nVjIzuwboAt7l\n7ufUej7lMLPngS53b4obsMzs28DD7n6bmQ0HDnL312o9r3KYWSvQC5zs7iXdbJuYTwTu/nPg1VrP\noxLc/SV33xh8/wfgV0BDrrDjKW8ED9uCr4b97cTMxgJ/DdxW67lIJjMbAbwXuB3A3Xc1ehAIzACe\nLTUIQIICQbMys4nAVOCx2s6kdMGllMeB7cC/u3vDngtwI/APwN5aT6RCHPipmW0wszm1nkyZjgD6\ngG8Gl+5uM7ODaz2pCrgIWF7OARQIGpiZvQP4IXC1u79e6/mUyt0H3P0EYCxwkpk15KU7MzsH2O7u\nG2o9lwo6zd2nAWcDnwwusTaqYcA04BvuPhV4E5hf2ymVJ7i8NRO4s5zjKBA0qOB6+g+B77r73bWe\nTyUEH9MfAv6q1nMpUTcwM7iu/n3gTDO7o7ZTKo+79wZ/bgfuAU6q7YzKsg3YlvaJ8y5SgaGRnQ1s\ndPffl3MQBYIGFCRYbwd+5e5frfV8ymFmHWY2Mvi+HfhL4Onazqo07r7A3ce6+0RSH9fXuPslNZ5W\nyczs4KAYgeASyvuAhq26c/eXgR4zmxwMzQAarsAiy8WUeVkIUh+VEsHMlgOnA6PNbBvweXe/vbaz\nKlk38D+BLcG1dYDPufsDNZxTqcYA3w4qH1qAFe7e8GWXTeKPgXtSv3cwDPieu/+ktlMq25XAd4NL\nKs8Bl9V4PiULgvNfAh8r+1hJKR8VEZFwujQkIpJwCgQiIgmnQCAiknAKBCIiCadAICKScAoE0rTM\nbCDozPikmd1pZgcNsf3nIh73eTMbHXW8HGY20cw+mPb4UjO7uZKvIaJAIM2s391PCLrN7gKuGGL7\nSIGgyiYCHxxqI5FyKBBIUjwMvBvAzC4J1kB43MxuDZreLQHag7HvBtutDJqtPVVsw7Ww1wjG3zCz\n64P1Fx41sz8OxicFj7eY2XVmNtiRdQnwP4LjfDoYO9zMfmJmvzGzL1Xg70YSToFAmp6ZDSPVk2WL\nmb0HuBDoDhrdDQB/5+7z2f8J4u+CXS939+mk1haYa2aHRHy90NcInj4YeNTdjwd+Dnw0GP8a8DV3\nn0KqJ86g+aT655/g7jcEYycEx58CXGhm44r6CxHJkpgWE5JI7WktOB4m1Z9pDjAdWBe0Tmgn1f46\nzFwz+5vg+3HAUcArEV53RoHX2AUMttDYQKpFAMCpwKzg++8BXy5w/NXuvhPAzH4JTAB6IsxLJJQC\ngTSz/uA38n2Chn3fdvcFhXY0s9OBvwBOdfe3zOxnwIERX7fQa+z2/X1dBijt/+Dbad+XegyRfXRp\nSJJmNXC+mR0KYGajzGxC8NzuoL03wAjgv4IgcDRwSoVeI59HgfOC7y9KG/8D8M4iXlukaAoEkijB\nus4LSa26tRn4d1IdUAGWAZuDZPFPgGFm9itSCdvIaygP8Rr5XA1cE2z/bmBnML4ZGAiSy5/Ou7dI\nGdR9VKQOBPc49Lu7m9lFwMXu/oFaz0uSQdcWRerDdODmIIfxGnB5jecjCaJPBCIiCaccgYhIwikQ\niIgknAKBiEjCKRCIiCScAoGISML9f/Vvvg3KfNtKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfQT2SS4a_rF",
        "colab_type": "text"
      },
      "source": [
        "### STEP 3 : Model Architecture \n",
        "\n",
        "\n",
        "###### Input : 4 \n",
        "###### 1 hidden Layer : 8 nodes\n",
        "###### Output : 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5wxUk3pT8z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = tf.keras.models.Sequential()\n",
        "network.add(tf.keras.layers.Dense(8, activation = 'relu', input_dim = 4))    # Input Layer\n",
        "network.add(tf.keras.layers.Dense(8, activation = 'relu'))    # Hidden Layer\n",
        "network.add(tf.keras.layers.Dropout(0.1))    # Dropout Layer\n",
        "network.add(tf.keras.layers.Dense(3, activation = 'softmax'))    # Output Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BJAG_IKa_rS",
        "colab_type": "text"
      },
      "source": [
        " ### STEP 4 : Compilation Step "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJmlutBKa_rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(loss='sparse_categorical_crossentropy', \n",
        "                optimizer='adam', \n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaSOa6Ja_rf",
        "colab_type": "text"
      },
      "source": [
        "### STEP 5 : Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUWVEV94a_ri",
        "colab_type": "code",
        "outputId": "1702ec7c-9c11-4d09-a4b5-f99bc78fd965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "target_labels = train_labels.replace(class_names, new_label)\n",
        "network.fit(train_features, target_labels, epochs=100)\n",
        "test_target_labels = test_labels.replace(class_names, new_label)\n",
        "test_loss, test_accuracy = network.evaluate(test_features, test_target_labels)\n",
        "print(\"Accuracy :\", test_accuracy)\n",
        "print(\"Losses :\", test_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "Train on 120 samples\n",
            "Epoch 1/100\n",
            "120/120 [==============================] - 0s 635us/sample - loss: 2.5296 - accuracy: 0.2833\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 2.2427 - accuracy: 0.3250\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 62us/sample - loss: 1.9679 - accuracy: 0.3500\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 68us/sample - loss: 1.6863 - accuracy: 0.3750\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 66us/sample - loss: 1.5056 - accuracy: 0.3417\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 1.5244 - accuracy: 0.3417\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 65us/sample - loss: 1.2999 - accuracy: 0.3333\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 68us/sample - loss: 1.1583 - accuracy: 0.3750\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 1.0753 - accuracy: 0.4083\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 69us/sample - loss: 1.1393 - accuracy: 0.5000\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 67us/sample - loss: 1.1918 - accuracy: 0.4500\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 66us/sample - loss: 1.1295 - accuracy: 0.4417\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 69us/sample - loss: 1.3437 - accuracy: 0.3833\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 66us/sample - loss: 1.1167 - accuracy: 0.4583\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 1.2267 - accuracy: 0.4667\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 65us/sample - loss: 0.9165 - accuracy: 0.5583\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 65us/sample - loss: 1.1472 - accuracy: 0.5500\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 61us/sample - loss: 1.1515 - accuracy: 0.4750\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 57us/sample - loss: 1.2325 - accuracy: 0.5333\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 61us/sample - loss: 1.0698 - accuracy: 0.5167\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 1.0267 - accuracy: 0.5167\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.9784 - accuracy: 0.4917\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.9571 - accuracy: 0.5083\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 1.2694 - accuracy: 0.4333\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 86us/sample - loss: 1.1526 - accuracy: 0.4917\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 1.0523 - accuracy: 0.5250\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 91us/sample - loss: 0.9929 - accuracy: 0.5250\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 68us/sample - loss: 1.1536 - accuracy: 0.4750\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 71us/sample - loss: 0.9800 - accuracy: 0.6333\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 70us/sample - loss: 0.9613 - accuracy: 0.6750\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 0.9900 - accuracy: 0.7167\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 0.8375 - accuracy: 0.7500\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 0.9282 - accuracy: 0.6917\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.9771 - accuracy: 0.7333\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 67us/sample - loss: 1.0513 - accuracy: 0.6833\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 71us/sample - loss: 1.0745 - accuracy: 0.6750\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.8507 - accuracy: 0.7750\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 89us/sample - loss: 0.9263 - accuracy: 0.7167\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.9056 - accuracy: 0.7667\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 1.0644 - accuracy: 0.6750\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 66us/sample - loss: 0.9263 - accuracy: 0.7167\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 69us/sample - loss: 0.9326 - accuracy: 0.7167\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.8564 - accuracy: 0.7417\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 75us/sample - loss: 0.8252 - accuracy: 0.7583\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 76us/sample - loss: 0.8903 - accuracy: 0.7583\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.9271 - accuracy: 0.6750\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 94us/sample - loss: 0.9744 - accuracy: 0.7167\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.8167 - accuracy: 0.7917\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 94us/sample - loss: 1.0102 - accuracy: 0.6667\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.7863 - accuracy: 0.7917\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 73us/sample - loss: 0.7736 - accuracy: 0.7750\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 84us/sample - loss: 0.7936 - accuracy: 0.7667\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 0.9872 - accuracy: 0.7000\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.8930 - accuracy: 0.7000\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.7274 - accuracy: 0.7750\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 81us/sample - loss: 0.9567 - accuracy: 0.7083\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 70us/sample - loss: 0.7990 - accuracy: 0.7333\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 73us/sample - loss: 0.8210 - accuracy: 0.7000\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.8451 - accuracy: 0.7167\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.7746 - accuracy: 0.7750\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 73us/sample - loss: 0.7595 - accuracy: 0.7333\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 70us/sample - loss: 0.7576 - accuracy: 0.7833\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 70us/sample - loss: 0.8484 - accuracy: 0.7417\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 76us/sample - loss: 0.7532 - accuracy: 0.7667\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 73us/sample - loss: 0.8592 - accuracy: 0.6500\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.6963 - accuracy: 0.7167\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.7359 - accuracy: 0.7250\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 69us/sample - loss: 0.8215 - accuracy: 0.7500\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 82us/sample - loss: 0.6328 - accuracy: 0.7667\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 62us/sample - loss: 0.7384 - accuracy: 0.7417\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 69us/sample - loss: 0.6778 - accuracy: 0.7667\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 76us/sample - loss: 0.6909 - accuracy: 0.8000\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.6874 - accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.6226 - accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.6166 - accuracy: 0.8000\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.6652 - accuracy: 0.7917\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.6366 - accuracy: 0.8083\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 78us/sample - loss: 0.6526 - accuracy: 0.7583\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 75us/sample - loss: 0.6050 - accuracy: 0.8167\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 68us/sample - loss: 0.6599 - accuracy: 0.7750\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.5891 - accuracy: 0.8250\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.6254 - accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 79us/sample - loss: 0.6529 - accuracy: 0.7833\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.5444 - accuracy: 0.8750\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 70us/sample - loss: 0.5403 - accuracy: 0.8167\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 65us/sample - loss: 0.5886 - accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.5878 - accuracy: 0.8250\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 64us/sample - loss: 0.6016 - accuracy: 0.7917\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.6343 - accuracy: 0.7417\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.6284 - accuracy: 0.8083\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 80us/sample - loss: 0.6297 - accuracy: 0.7583\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 65us/sample - loss: 0.5463 - accuracy: 0.7833\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 79us/sample - loss: 0.5841 - accuracy: 0.7833\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 79us/sample - loss: 0.5765 - accuracy: 0.8000\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 76us/sample - loss: 0.6263 - accuracy: 0.7500\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 81us/sample - loss: 0.4836 - accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.5422 - accuracy: 0.8167\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.5930 - accuracy: 0.8000\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 80us/sample - loss: 0.5400 - accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 88us/sample - loss: 0.5704 - accuracy: 0.7833\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "30/30 [==============================] - 0s 925us/sample - loss: 0.4510 - accuracy: 0.8000\n",
            "Accuracy : 0.8\n",
            "Losses : 0.45097655057907104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZKA7BEDa_rp",
        "colab_type": "text"
      },
      "source": [
        "### STEP 6 : Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPiyIr5da_rt",
        "colab_type": "code",
        "outputId": "db192cc3-4937-4c49-859c-52cec7887828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions = network.predict(test_features)\n",
        "i = 0\n",
        "for values in predictions:\n",
        "  i += 1\n",
        "  print(\"Predictions {}\".format(i))\n",
        "  print(f\"{class_names[0]} : {values[0] * 100.0}%\")\n",
        "  print(f\"{class_names[1]} : {values[1] * 100.0}%\")\n",
        "  print(f\"{class_names[2]} : {values[2] * 100.0}%\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "Predictions 1\n",
            "Setosa : 84.37005877494812%\n",
            "Versicolor : 14.56388384103775%\n",
            "Virginica : 1.0660508647561073%\n",
            "Predictions 2\n",
            "Setosa : 5.938209220767021%\n",
            "Versicolor : 44.08280551433563%\n",
            "Virginica : 49.97898042201996%\n",
            "Predictions 3\n",
            "Setosa : 86.4290714263916%\n",
            "Versicolor : 12.958478927612305%\n",
            "Virginica : 0.6124514155089855%\n",
            "Predictions 4\n",
            "Setosa : 0.6319213658571243%\n",
            "Versicolor : 24.911074340343475%\n",
            "Virginica : 74.45700764656067%\n",
            "Predictions 5\n",
            "Setosa : 26.454192399978638%\n",
            "Versicolor : 49.852538108825684%\n",
            "Virginica : 23.693260550498962%\n",
            "Predictions 6\n",
            "Setosa : 2.1675098687410355%\n",
            "Versicolor : 33.42971205711365%\n",
            "Virginica : 64.40277695655823%\n",
            "Predictions 7\n",
            "Setosa : 81.53175115585327%\n",
            "Versicolor : 16.94646030664444%\n",
            "Virginica : 1.5217862091958523%\n",
            "Predictions 8\n",
            "Setosa : 1.6633657738566399%\n",
            "Versicolor : 30.372995138168335%\n",
            "Virginica : 67.96364188194275%\n",
            "Predictions 9\n",
            "Setosa : 6.33886530995369%\n",
            "Versicolor : 46.76213264465332%\n",
            "Virginica : 46.89900279045105%\n",
            "Predictions 10\n",
            "Setosa : 8.783406764268875%\n",
            "Versicolor : 47.486382722854614%\n",
            "Virginica : 43.73020827770233%\n",
            "Predictions 11\n",
            "Setosa : 4.755454882979393%\n",
            "Versicolor : 42.026329040527344%\n",
            "Virginica : 53.218209743499756%\n",
            "Predictions 12\n",
            "Setosa : 84.22223329544067%\n",
            "Versicolor : 14.783835411071777%\n",
            "Virginica : 0.9939251467585564%\n",
            "Predictions 13\n",
            "Setosa : 84.5497190952301%\n",
            "Versicolor : 14.523167908191681%\n",
            "Virginica : 0.9271210059523582%\n",
            "Predictions 14\n",
            "Setosa : 7.956001162528992%\n",
            "Versicolor : 46.488237380981445%\n",
            "Virginica : 45.55576741695404%\n",
            "Predictions 15\n",
            "Setosa : 13.247726857662201%\n",
            "Versicolor : 56.36632442474365%\n",
            "Virginica : 30.385953187942505%\n",
            "Predictions 16\n",
            "Setosa : 1.9631234928965569%\n",
            "Versicolor : 32.84210562705994%\n",
            "Virginica : 65.19476771354675%\n",
            "Predictions 17\n",
            "Setosa : 1.0517101734876633%\n",
            "Versicolor : 23.36418777704239%\n",
            "Virginica : 75.58410167694092%\n",
            "Predictions 18\n",
            "Setosa : 84.17229056358337%\n",
            "Versicolor : 14.841537177562714%\n",
            "Virginica : 0.9861691854894161%\n",
            "Predictions 19\n",
            "Setosa : 1.280816737562418%\n",
            "Versicolor : 25.69407820701599%\n",
            "Virginica : 73.02510738372803%\n",
            "Predictions 20\n",
            "Setosa : 86.00471019744873%\n",
            "Versicolor : 13.224072754383087%\n",
            "Virginica : 0.7712169550359249%\n",
            "Predictions 21\n",
            "Setosa : 5.560046434402466%\n",
            "Versicolor : 44.42293047904968%\n",
            "Virginica : 50.017017126083374%\n",
            "Predictions 22\n",
            "Setosa : 85.47255396842957%\n",
            "Versicolor : 13.674098253250122%\n",
            "Virginica : 0.8533465676009655%\n",
            "Predictions 23\n",
            "Setosa : 9.398219734430313%\n",
            "Versicolor : 54.543107748031616%\n",
            "Virginica : 36.05867326259613%\n",
            "Predictions 24\n",
            "Setosa : 1.0572856292128563%\n",
            "Versicolor : 25.46272873878479%\n",
            "Virginica : 73.4799861907959%\n",
            "Predictions 25\n",
            "Setosa : 4.66238297522068%\n",
            "Versicolor : 43.932196497917175%\n",
            "Virginica : 51.40541195869446%\n",
            "Predictions 26\n",
            "Setosa : 83.36113095283508%\n",
            "Versicolor : 15.459215641021729%\n",
            "Virginica : 1.1796541512012482%\n",
            "Predictions 27\n",
            "Setosa : 0.953440647572279%\n",
            "Versicolor : 30.302652716636658%\n",
            "Virginica : 68.74390244483948%\n",
            "Predictions 28\n",
            "Setosa : 13.698618113994598%\n",
            "Versicolor : 55.22157549858093%\n",
            "Virginica : 31.07980489730835%\n",
            "Predictions 29\n",
            "Setosa : 85.06840467453003%\n",
            "Versicolor : 14.025349915027618%\n",
            "Virginica : 0.9062490426003933%\n",
            "Predictions 30\n",
            "Setosa : 4.234648868441582%\n",
            "Versicolor : 38.299134373664856%\n",
            "Virginica : 57.46621489524841%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeJhN_m4zZ-7",
        "colab_type": "code",
        "outputId": "603b626d-c8a9-4689-b72c-c0471ba8e915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "predict_value = 6\n",
        "print(\"Predictions\")\n",
        "print(f\"{class_names[0]} : {predictions[predict_value][0] * 100.0}%\")\n",
        "print(f\"{class_names[1]} : {predictions[predict_value][1] * 100.0}%\")\n",
        "print(f\"{class_names[2]} : {predictions[predict_value][2] * 100.0}%\")\n",
        "def predict_ans(x, y):\n",
        "  if (x[0] > x[1]) & (x[0] > x[2]):\n",
        "    result = y[0]\n",
        "  elif (x[1] > x[0]) & (x[1] > x[2]):\n",
        "    result = y[1]\n",
        "  else:\n",
        "    result = y[2]\n",
        "  return result\n",
        "print(\"\\nPredicted Answer\")\n",
        "print(predict_ans(predictions[predict_value], class_names))\n",
        "print(\"\\nActual Answer\")\n",
        "print(f\"{test_labels.iloc[predict_value]}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions\n",
            "Setosa : 81.53175115585327%\n",
            "Versicolor : 16.94646030664444%\n",
            "Virginica : 1.5217862091958523%\n",
            "\n",
            "Predicted Answer\n",
            "Setosa\n",
            "\n",
            "Actual Answer\n",
            "Setosa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdZgcwE61Ko2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}